# CovidDocumentClustering
Our project will use TF-IDF feature extraction and different clustering algorithms to cluster COVID-19 research publications of similar topics in order to simplify the search for related articles, which would be useful when dealing with continuous new information on the virus. This project will use the CORD-19 dataset, and the evaluation will consist of internal validity indices, such as the Silhouette index to measure cluster scatter/separation, and external validity indices using the metadata provided in the dataset.

### Team Members:
Chelsea Pan, cjpan@uci.edu; 
Vivian Zhang, vszhang@uci.edu

## Data Set
This project utilized the COVID-19 Open Research Dataset (CORD-19), which can be found at https://www.semanticscholar.org/cord19. The data set has more than 300,000 coronavirus articles, which are presented as structured full text papers, and includes a rich collection of metadata. 

The dataset is updated regularly as new research is published in different services. We downloaded the 02-01-2021 version of the set, so any publications after Feb. 1 are not included in our data. In the downloaded folder, a metadata CSV file is provided along with a folder of full text JSON document parses.

The metadata CSV file contains all the important and relevant information needed about each document. Each row is identified by a unique ID, and contains the path in the document parses folder to retrieve the full text for that document. We mainly used the metadata file to go through all the documents, find the corresponding JSON path, and extract the body text to preprocess and add to our corpus.

After performing preprocessing on the document, it seemed that some entries in metadata were missing the JSON paths for the full document text, so we were actually only working with a little over 158,000 documents from the dataset in regards to the entire document. Because each document was considerably long with many tokens, the processing and vectorizing time took extremely long, not to mention that we would be training multiple models with that large number of extracted features. Therefore, we also attempted using the abstracts of the data rather than the whole document text, in which we were able to extract over 318,000 abstracts. This sped up the extraction process, since the abstract could be accessed straight out of the metadata file. Even though there were more data pieces to work with (over twice as many), the size of each was much smaller, and the models could be trained on a larger set and wider variety of documents.

## Technical Approach
Before we began clustering, we preprocessed the dataset through tokenization (dividing texts into terms), removing stop words (removing words with no semantic meaning such as the, a, etc.), removing noise (such as words not clearly recognized as English), and stemming (reducing a word to its root). We decided to also perform lemmatization, which is similar to stemming, but uses lexical knowledge to get the correct base form of words instead of simply chopping off inflections. These were done through the use of Python NLP libraries such as scikit-learn.

Feature extraction was our next step after the document corpus was prepared; we mapped text data to vectors with TF-IDF, with higher weights being given to words seen frequently in a specific document but less so in the overall corpus (to adjust for the fact that some words occur more in general). This is done through scikit-learn’s TfidfVectorizer in its feature_extraction package. 

Our baseline method for our project was k-means clustering. This algorithm takes in TF-IDF vectors of the documents as its input, and its output will classify the dataset into k clusters based on “similarity.” The idea begins with k centroids that represent cluster centers, and each iteration recalculates distances between those centroids and data points to update the centroid locations. Since we initially don’t know the ideal number of clusters, we experimented with different k-values to determine which to use for our final model. However, choosing the best k-value can be tricky since it is a very subject analysis, and will be explained further in the Experimentation and Evaluation section. 

Our advanced method was hierarchical clustering, and there are two options for this method. One is top-down divisive clustering, where all documents start in one big cluster, and the algorithm recursively splits it into smaller clusters. The second method is bottom-up agglomerative clustering, where each single document starts in its own singleton cluster, and the clusters are merged into larger clusters. We utilized the second method, as it more typically used in information retrieval and is conceptually less complex. When merging, there are several choices to determine our measure of “similarity” or distance, such as single-link (similarity between most similar members), complete-link (similarity between most different members), etc. Through some research, it was discovered that single and complete-link both produce undesirable clusters, so it is more likely that we will experiment with average-link or centroid-similarity. Average-link computes the average similarity between all pairs of documents, including those in the same cluster; centroid-similarity computes the similarity of the clusters’ centroids. 

Another point to consider is that hierarchical clustering is significantly slower and more complex, so using our large dataset with hundreds of thousands of documents is an issue. We considered performing this clustering algorithm on a subset of the abstracts, around 20,000 or less, given an appropriate k-value that we obtain from k-means experimentation. A benefit of agglomerative clustering is that it comes in the form of a visually appealing dendrogram, which is very useful for interpretation. We can make use of where clusters merge and which clusters are more related to one another to make better sense of our data.

After successfully implementing and evaluating both k-means and agglomerative clustering, we analyzed the two using t-SNE visualization and subjective analysis with provided metadata. Such techniques and evaluation will be discussed further in depth in the Experimentation and Evaluation section.

## Experiments and Evaluation
When it comes to clustering algorithms, special evaluation techniques must be applied because unsupervised learning does not have target variables to calculate the “accuracy” of the model. Two kinds of validity indices can be used: external and internal. External indices make use of a known cluster structure of a dataset; in our case, we used the metadata given in our dataset, specifically the journal of each article, to evaluate how meaningful our clusters were given the variety of journals that the documents came from. Internal indices utilize features inherent in the data set, such as evaluating the distance between clusters or the compactness of each cluster. Internal indices are usually what help decide the appropriate number of clusters to use in our algorithm.

We experimented with different numbers of clusters for our K-means algorithm and evaluated the results using the “elbow method” and the Silhouette index. The Silhouette index interprets the consistency within clusters and is a form of internal validity measure. The score ranges between -1 and 1: a score of 1 suggests the means clusters are clearly distinguishable and well apart from each other, a score of 0 suggests the distance between clusters is insignificant, and a score of -1 indicates wrong assignments of means clusters. 

*SIL_INDEX = ( a - b ) / max(a, b), where (a) is the mean intra-cluster distance and (b) is the mean nearest-cluster distance.

What we found was that the scoring performed pretty badly among all clusters: the scores ended up really close to 0, and most were under 0.02. We believe it to be related to the difficulty of quantifying document words accurately, and possible the large size of the feature vector. Instead of heavily depending on the Silhouette index to determine our optimal k-value, we carried out some experiment on dummy datasets that we created ourselves to get a better understanding of the scoring concept. We mainly turned to the elbow method to determine the best number of clusters to use.

The elbow method is a simple, yet informative way of evaluating the optimal number of clusters. In our project, we plotted the cost function (within-cluster sum of squares) with respect to number of clusters. The “elbow” of the graph signifies that the addition of a cluster doesn’t significantly increase the cost, so the optimal number of clusters is chosen at that position. To find the optimal k, we calculated the sum of squared distances from range 5 to 50 with steps of 5 and graphed the results. To do a little more fine tuning on the "best" k, we trained 6 k-means models with k = 20, 21, 22, 23, 24, and 25 and graphed the elbow of these 6 models. Based on the results of this graph, we arbitrarily chose the k value 23 to move forward with in our model.

Next, we experimented with visualizing the k-means clusters in a scatter plot. We tested two different dimensionality reduction methods to help with plotting: principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE). Both are fairly common methods of reducing the dimensionality of a dataset. In the case of PCA, we reduced the dimensions while keeping a 95% variance. This way, we obtained lower dimensions to our data while preserving as much of the information as possible. In graphing, we graphed both the fully reduced PCA dataset as well as a subset that more clearly showed individual clusters.

For t-SNE, we reduced our high dimensional data into a two dimensional plane to make a more visually comprehensive plot of the data in x and y coordinates. Once again, we graphed both the fully reduced dataset as well as a subset that more clearly showed individual clusters.

After choosing 23 as the best k-value for our dataset, we trained a clustering model and performed further analysis on the produced clusters. We used bar graphs to show the most common words in each cluster, so it’s easy to identify any themes going on in each cluster. This can also judge how well the clusters are separated. Not only did we focus on the words within the documents, but we also utilized the journal of each document provided in our metadata as a means of external validation. With meaningful clustering, we expected to find some sort of correlation with the journals that the abstracts are from. For example, one cluster contains a majority of documents from Journal A, while another cluster contains a lot of documents from Journal B. We hoped to see a majority of documents in each cluster to come from fewer number journals, whereas we expect a much bigger variety of journals from random clustering.

After creating the 23 clusters using k-means clustering, we calculate the following metrics for each cluster: (1) the number of documents, (2) the top journal that the most number of documents came from, (3) the percentage of documents that came from the top journal, and (4) the number of different journals that made up 50% of the documents.

The results were surprising because we also graphed the top few journals in each cluster, and the top journal was significantly more prevalent than the others in the graphs, so we thought the percentage of documents coming from the top journal would be much higher. We realized that there are actually many different journals, and many documents in each cluster are unique in regards to the journal it came from, so the percentage is affected. However, this doesn’t invalidate the fact that the top journal still dominates a significant portion of the cluster. To confirm our idea, we performed the same kind of journal analysis with random clustering results.

We did two trials with random labeling -- in the first trial, we simply gave each document in our dataset a random label between 0 to 22 inclusive (to make 23 clusters), and naturally, the random generator distributes the random integers pretty evenly. Therefore, the number of documents in each cluster fell between 800 to 900, affecting the influence of each document and the corresponding journal. The percentage of the top journal was higher than that found in the k-means results, which initially made us believe that our k-means algorithm initially performed worse than random clustering, but there was a significantly bigger variety of journals in each cluster that made up 50%.

In order to control each document’s influence on the metrics, we did a second trial where we assigned the documents randomly to one of the 23 clusters, but used the same number of documents as the results of the k-means clustering. For example, if there were 400 documents in Cluster 5 from the k-means clustering results, then we randomly assigned 400 documents to Cluster 5 in this trial. In this trial, we saw more expected results, in which the percentage of the top journal was generally lower than that of k-means results, and there was a much bigger variety in journals making up 50% of each cluster. This evaluation helped confirm that k-means was making steps toward meaningful clustering, but we then utilized hierarchical clustering as a means of both evaluation and comparison.

Our main use for hierarchical clustering was producing a dendrogram with 23 leaf nodes to represent the 23 clusters. Not only can we compare this clustering with the clusters produced from k-means through evaluation, but we can also analyze the similarity between the resulting clusters by seeing where they merge in the dendrogram. 

Using these clusters, we once again performed cluster analysis and journal evaluation, where we graphed the most common words in each cluster, and put the metrics in a table regarding number of documents, prevalence of the top journal, etc. We found that on average, the top journal of each cluster dominated more documents and less journals made up 50% of the clusters, suggesting overall better performance than what we saw in k-means clustering. Using the number of documents in the table, we could match which cluster corresponded to which leaf node in the diagram. We listed out the clusters that were in the orange group, the green group, and the red group. Then using the graphs showing the most common words in each cluster, we could extract the general theme behind each grouping. There was one cluster that really stood out from the rest and was merged extremely late in the tree as its own cluster. We matched this to be Cluster 16, and there was not much meaning that could be derived from the most common words within this cluster. We figured that this cluster contained articles with more extraneous words, so we paid more attention to the three colored groupings.

We found that the most common words in the clusters of the orange group were focused on biological aspects, containing words such as virus, vaccine, drug, and protein. The green group was more medical related, containing words like patient, cancer, treatment, and surgery. The red group was more focused on healthcare, containing words like mask, mental health, patient, student, and care. The dendrogram shows red and green clusters to merge before orange, which subjectively makes sense since healthcare and the medical field are more closely related than biological factors of the virus. 

## Insights and Lessons Learned
One of the main challenges we ran into was setting up the environment in Jupyter Notebook under the assumption that we would train a model using TensorFlow GPU. We had issues installing versions of CUDA and cuDNN that were compatible with each other, especially given that we all have different machines and different graphics cards that have varying processing power. As it turns out, we ended up using sklearn modules for both our k-means and hierarchical algorithms and did not need to make use of CUDA to train our models. If we had known that we would have had to run our code on our CPU either way, we could have saved time and made progress on our hierarchical algorithm weeks earlier.

Due to multiple factors, time posed as a significant challenge when it came to processing the data and training the models. None of us fully predicted how long it would take to process a massive dataset because it was our first time applying machine learning to a text processing project. Since running our models on 300,000+ documents took an extremely long time, we compromised by using only the preprocessed abstracts from the dataset. This cut down our run time significantly and gave us more time to experiment with our algorithms. Even with this smaller data set, our models still took a very long time to run, so we made use of subsets to test out our algorithms before committing to a full run through. We didn't prepare well enough to combat our issues with run time, and that ended up hindering a lot of our experimentation and results.

Our last significant challenge was finding ways to evaluate our data. Our original plan was to use the Silhouette index for internal evaluation and semi-supervised clustering for external evaluation. However, because time became our biggest constraint, we ended up using journal clustering as our form of external evaluation, since semi-supervised would have taken too long to make guaranteed progress. We resorted to using journals because the metadata was rather scarce and did not have key terms for us to utilize. If we were in charge of a research lab, we would have spent some time exploring semi-supervised clustering as a form of evaluation, as it would have been interesting to see the results.

For project insights, despite using the same sklearn k-means model with similar k values and t-SNE dimensionality reduction as the University of Maryland research group from our reference project, we were surprised that we obtained wildly different results in our scatterplots. This could be due to them running the model on a much earlier iteration of the dataset that had only 50,000 documents. Our model was run on 318,000 document abstracts instead of full documents, and our clusters were not as clean as the plot we were comparing ours to. 

In the future, it would be worth looking into why our results varied so wildly. One avenue of experimentation could be running a smaller subset of 50,000 full documents with k-means to compare if the clusters are more similar when we imitate their process with the same sized dataset. Depending on the results of that experiment, we could have used that as a control scatterplot for when we scaled up our number of documents. Ideally, if we had both the time and processing power, it would have been great to be able to run k-means on full document datasets incrementing from 50,000 to the full dataset to see how the algorithm changes with dataset size. It also could be the case that clustering our 318,000 document abstracts was beyond the limits of our k-means model, as k-means is one of the simplest clustering algorithms. If we had more time, we could also experiment with the results of other clustering algorithms to find the most successful and efficient method of clustering. However, one other algorithm that we had the chance to explore within the timeline of this project was agglomerative hierarchical clustering.

For agglomerative clustering, the run time was significantly slower due to higher complexity. For a large set of n documents, the complexity could be as slow as O(n^3). Because it would have been impractical to run this algorithm on the full dataset, we followed the professor's suggestion of running agglomerative clustering on the centroids of our k-means clusters. Our results with this method met our expectations. First of all, we expected hierarchical clustering to perform better in general in regards to journal evaluation because of the overall complexity of the algorithm, which was the case in our output. Second, using the dendrogram we can analyze which clusters merged when, indicating the similarity between clusters. We expected clusters that focused more on biological factors to be merged together earlier, clusters that focus more on medical factors to be merged together earlier, and that these two big clusters would be merged last, which is what we found using our word analysis.


### Publicly Available Code
Programming Languages & Libraries
- Python 3.8, pickle, NumPy

Data Processing Modules
- NLTK, JSON, sklearn.feature_extraction.text ENGLISH_STOP_WORDS and TfidfVectorizer to preprocess data set
- sklearn.decomposition PCA and sklearn.manifold TSNE for dimensionality reduction in plotting results

Clustering Algorithms
- sklearn.cluster KMeans for our simple model
- sklearn.cluster AgglomerativeClustering for our more complex model

Evaluation Modules
- sklearn.metrics silhouette_score, scipy.spatial.distance cdist to calculate sum of squared distances for elbow graph

Data Visualization
- Matplotlib, WordCloud, Pandas, scipy.cluster.hierarchy dendrogram to output scatter plots, word clouds, data frames, and hierarchical clustering dendrograms

